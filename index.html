<!doctype html>
<html lang="ja">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<title>PWA リアルタイム・ボイスチェンジャー — AudioWorklet版</title>
<meta name="theme-color" content="#1e293b">
<style>
  body{font-family:system-ui,-apple-system,Segoe UI,Roboto,'Hiragino Kaku Gothic ProN',Meiryo; background:#071021;color:#e6eef6;margin:0;display:flex;align-items:center;justify-content:center;height:100vh}
  .card{width:980px;max-width:95%;background:linear-gradient(180deg,rgba(255,255,255,0.02),rgba(0,0,0,0.06));padding:18px;border-radius:12px}
  h1{margin:0 0 8px;font-size:18px}
  .row{display:flex;gap:8px;align-items:center;margin:8px 0}
  label{font-size:13px;color:#cfe6ff}
  input[type=range]{width:220px}
  button{padding:8px 12px;border-radius:8px;border:0;background:#0ea5a4;color:#012;background:#0ea5a4;color:white;font-weight:600;cursor:pointer}
  button.ghost{background:transparent;border:1px solid rgba(255,255,255,0.06)}
  .status{font-size:13px;color:#9fb7cf}
</style>
</head>
<body>
  <div class="card">
    <h1>リアルタイム・ボイスチェンジャー（AudioWorklet版）</h1>
    <p class="status">ボタンを押してマイクを許可すると、AudioWorklet による低遅延ピッチシフトで再生します。Safari / iOS の場合は AudioWorklet があるか確認され、なければ ScriptProcessor にフォールバックします。</p>

    <div class="row">
      <button id="btnStart">Start</button>
      <button id="btnStop" class="ghost" disabled>Stop</button>
      <div style="margin-left:auto;">状態: <span id="state">停止中</span></div>
    </div>

    <div class="row">
      <label>ピッチ（半音） <span id="pval">0</span></label>
      <input type="range" id="pitch" min="-12" max="12" value="0">
      <label style="margin-left:18px">深さ（wet） <span id="wval">1.00</span></label>
      <input type="range" id="wet" min="0" max="1" step="0.01" value="1">
    </div>

    <div class="row">
      <label>グラニュール幅(ms)</label>
      <input type="range" id="grain" min="10" max="200" value="60">
      <span id="gval">60</span>ms
    </div>

    <div style="margin-top:12px;font-size:12px;color:#9fb7cf">
      注: これは教育目的のシンプル実装です。高品質化（位相補正、FFT法）は別途対応できます。
    </div>
  </div>

<script>
// --- UI
const btnStart = document.getElementById('btnStart');
const btnStop = document.getElementById('btnStop');
const stateEl = document.getElementById('state');
const pitchEl = document.getElementById('pitch');
const pval = document.getElementById('pval');
const wetEl = document.getElementById('wet');
const wval = document.getElementById('wval');
const grainEl = document.getElementById('grain');
const gval = document.getElementById('gval');

pval.textContent = pitchEl.value;
wval.textContent = Number(wetEl.value).toFixed(2);
gval.textContent = grainEl.value;

pitchEl.addEventListener('input', ()=> pval.textContent = pitchEl.value);
wetEl.addEventListener('input', ()=> wval.textContent = Number(wetEl.value).toFixed(2));
grainEl.addEventListener('input', ()=> gval.textContent = grainEl.value);

// --- Audio graph globals
let audioContext = null;
let micStream = null;
let sourceNode = null;
let gainDry = null, gainWet = null, merger = null;
let workletNode = null;
let scriptNode = null;

async function startAudio() {
  if (audioContext) return;
  audioContext = new (window.AudioContext || window.webkitAudioContext)();
  await audioContext.resume();

  // create common nodes
  gainDry = audioContext.createGain(); gainDry.gain.value = 0.0; // initial dry muted
  gainWet = audioContext.createGain(); gainWet.gain.value = Number(wetEl.value);
  merger = audioContext.createGain();
  gainDry.connect(merger);
  gainWet.connect(merger);
  merger.connect(audioContext.destination);

  try{
    // try AudioWorklet first
    const processorCode = `
    class GranularPitchProcessor extends AudioWorkletProcessor {
      static get parameterDescriptors() {
        return [
          { name: 'rate', defaultValue: 1 },
          { name: 'grainMs', defaultValue: 60 }
        ];
      }
      constructor(){
        super();
        this.sampleRate = sampleRate;
        this.maxBuf = Math.floor(this.sampleRate * 4.0); // 4s buffer
        this.buf = new Float32Array(this.maxBuf);
        this.writePos = 0;
        this.readPos = 0;
        this.windowPos = 0;
        this.port.onmessage = (e)=>{
          if (e.data === 'clear') { this.buf.fill(0); this.writePos = this.readPos = 0; }
        };
      }
      // linear interp helper
      interp(buf, idx){
        const i0 = Math.floor(idx) % this.maxBuf;
        const i1 = (i0 + 1) % this.maxBuf;
        const frac = idx - Math.floor(idx);
        return buf[i0]* (1-frac) + buf[i1]*frac;
      }
      process(inputs, outputs, parameters) {
        const input = inputs[0];
        const output = outputs[0];
        if (!input || input.length === 0) return true;
        const inCh = input[0];
        const outCh = output[0];
        const rateVals = parameters.rate;
        const grainVals = parameters.grainMs;
        for (let i=0;i<inCh.length;i++){
          // write input into circular buffer
          this.buf[this.writePos] = inCh[i];
          this.writePos = (this.writePos + 1) % this.maxBuf;

          // determine rate (may be constant or per-sample)
          const rate = rateVals.length>1 ? rateVals[i] : rateVals[0];
          // read position moves with rate
          this.readPos += rate;
          if (this.readPos >= this.maxBuf) this.readPos -= this.maxBuf;

          // output via linear interp
          outCh[i] = this.interp(this.buf, this.readPos);
        }
        return true;
      }
    }
    registerProcessor('granular-pitch-processor', GranularPitchProcessor);
    `;

    const blob = new Blob([processorCode], {type: 'application/javascript'});
    const url = URL.createObjectURL(blob);
    await audioContext.audioWorklet.addModule(url);

    workletNode = new AudioWorkletNode(audioContext, 'granular-pitch-processor', {
      numberOfInputs: 1, numberOfOutputs: 1, outputChannelCount: [1], parameterData: { rate: 1 }
    });

    workletNode.connect(gainWet);

    // connect mic directly to worklet; we'll also provide dry path
    micStream = await navigator.mediaDevices.getUserMedia({audio:true});
    sourceNode = audioContext.createMediaStreamSource(micStream);
    sourceNode.connect(workletNode);
    sourceNode.connect(gainDry);

    // update parameters on UI change
    function updateParams(){
      const semis = Number(pitchEl.value);
      const rate = Math.pow(2, semis/12);
      workletNode.parameters.get('rate').setValueAtTime(rate, audioContext.currentTime);
      const grainMs = Number(grainEl.value);
      workletNode.parameters.get('grainMs').setValueAtTime(grainMs, audioContext.currentTime);
      gainWet.gain.value = Number(wetEl.value);
      gainDry.gain.value = 1 - Number(wetEl.value);
    }
    pitchEl.addEventListener('input', updateParams);
    wetEl.addEventListener('input', updateParams);
    grainEl.addEventListener('input', updateParams);

    updateParams();

    stateEl.textContent = '再生中 (AudioWorklet)';
    btnStart.disabled = true; btnStop.disabled = false;

  } catch(err){
    console.warn('AudioWorklet unavailable or failed — falling back', err);
    // fallback to ScriptProcessorNode approach (less ideal but Safari compatible)
    micStream = await navigator.mediaDevices.getUserMedia({audio:true});
    sourceNode = audioContext.createMediaStreamSource(micStream);
    const bufferSize = 2048;
    scriptNode = audioContext.createScriptProcessor(bufferSize,1,1);
    // small circular buffer
    const maxBuf = audioContext.sampleRate * 4;
    const buf = new Float32Array(maxBuf);
    let writePos = 0; let readPos = 0; // readPos will move at rate

    scriptNode.onaudioprocess = function(ev){
      const inData = ev.inputBuffer.getChannelData(0);
      const outData = ev.outputBuffer.getChannelData(0);
      // write
      for (let i=0;i<inData.length;i++){
        buf[writePos] = inData[i];
        writePos = (writePos + 1) % maxBuf;
      }
      // update rate from UI
      const semis = Number(pitchEl.value);
      const rate = Math.pow(2, semis/12);
      const wet = Number(wetEl.value);
      for (let i=0;i<outData.length;i++){
        // linear interp read
        const i0 = Math.floor(readPos) % maxBuf;
        const i1 = (i0 + 1) % maxBuf;
        const frac = readPos - Math.floor(readPos);
        const sample = buf[i0] * (1-frac) + buf[i1] * frac;
        outData[i] = sample * wet;
        readPos += rate;
        if (readPos >= maxBuf) readPos -= maxBuf;
      }
    };
    sourceNode.connect(scriptNode);
    scriptNode.connect(gainWet);
    sourceNode.connect(gainDry);

    // update wet/dry on UI
    wetEl.addEventListener('input', ()=>{ gainWet.gain.value = Number(wetEl.value); gainDry.gain.value = 1-Number(wetEl.value); });
    pitchEl.addEventListener('input', ()=>{});
    grainEl.addEventListener('input', ()=>{});

    stateEl.textContent = '再生中 (ScriptProcessor fallback)';
    btnStart.disabled = true; btnStop.disabled = false;
  }
}

function stopAudio(){
  if (!audioContext) return;
  try{
    if (workletNode) { workletNode.disconnect(); workletNode.port.postMessage('clear'); workletNode = null; }
    if (scriptNode) { scriptNode.disconnect(); scriptNode = null; }
    if (sourceNode) { sourceNode.disconnect(); sourceNode = null; }
    if (micStream) { micStream.getTracks().forEach(t=>t.stop()); micStream = null; }
    audioContext.close(); audioContext = null;
    stateEl.textContent = '停止中';
    btnStart.disabled = false; btnStop.disabled = true;
  } catch(e){ console.warn(e); }
}

btnStart.addEventListener('click', async ()=>{
  try{
    await startAudio();
  } catch(err){
    alert('マイクが取得できませんでした: ' + err.message);
  }
});
btnStop.addEventListener('click', ()=> stopAudio());

</script>
</body>
</html>
